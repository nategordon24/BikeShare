library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read_csv(drug_consumption.csv)
library(tidyverse)
data <- read_csv(drug_consumption.csv)
qnorm(0.025)
qnorm(0.025,0.5,0.25/50)
qnorm(0.025,0.5,sqrt(0.25/50))
qnorm(0.975,0.5,sqrt(0.25/50))
pnorm(.361,0.1,sqrt(0.09/50))
1-pnorm(0.639,0.1,sqrt(0.09/50))
pnorm(.361,0.3,sqrt(0.21/50))
pnorm(.361,0.3,sqrt(0.21/50)) + (1-pnorm(0.639,0.3,sqrt(0.21/50)))
pnorm(.361,0.2,sqrt(0.16/50)) + (1-pnorm(0.639,0.2,sqrt(0.16/50)))
qnorm(0.025,0,1)
qnorm(0.025,0,sqrt(1/5))
qnorm(0.975,0,sqrt(1/5))
pnorm(7.66836)
pnorm(7.66)
pnorm(5)
(129.4538-128)/(2.1/sqrt(40))
pnorm(4.3783999)
1-pnorm(4.37)
(129.4538-128)/(2.1/sqrt(40))
pnorm(4.378399)
1-0.999994
1-0.999994)*10
(128.6-128)/(2.1/sqrt(40))
pnorm(1.807016)
pnorm(-1.645)
1-pnorm(4.378)
1-pnorm(4.378)*10
(1-pnorm(4.378))*10
((1-pnorm(4.378))*10)*10
pnorm(-0.4424885)
1-0.3290679
pnorm(-0.4424885)
1-pt(4.378,39)
10*(1-pt(4.378,39))
10*(1-pt(4.38,39))
1-pt(4.568,9)
qnorm(0.01)
qnorm(0.005)
qt(0.005,9)
qt(0.01,9)
pbinom(12,20,0.4)
pbinom(12,20,0.5)
pbinom(12,20,0.6)
pbinom(12,20,0.7)
pbinom(12,20,0.9)
pbinom(12,20,1)
pnorm(0.05)
qnorm(0.05)
1.644854*(sqrt(5/20))
pnorm(7.5,7,sqrt(5))
pnorm(7.5,7,sqrt(5/20))
pnorm(7.5,7.82,sqrt(5/20))
pnorm(8,7.82,sqrt(5/20))
pnorm(8.5,7.82,sqrt(5/20))
pnorm(9,7.82,sqrt(5/20))
qnorm(0.05)
qnorm(0.8)
pnorm(8,7.82,sqrt(5/20))
pnorm(8,7.82,sqrt(5/30))
pnorm(8,7.82,sqrt(5/50))
pnorm(8,7.82,sqrt(5/70))
pnorm(8,7.82,sqrt(5/80))
pnorm(8,7.82,sqrt(5/90))
pnorm(8,7.82,sqrt(5/100))
pnorm(8,7.82,sqrt(5/125))
pnorm(8,7.82,sqrt(5/120))
pnorm(8,7.82,sqrt(5/110))
pnorm(8,7.82,sqrt(5/100))
pnorm(8,7,sqrt(5/31))
# Set up a sequence of theta values between 0 and 1
theta <- seq(0, 1, length.out = 500)
# Compute the density of the Beta(6, 4) distribution
prior_density <- dbeta(theta, shape1 = 6, shape2 = 4)
# Plot the prior density
plot(theta, prior_density, type = "l", lwd = 2, col = "blue",
main = "Prior Density: Beta(6, 4)",
xlab = expression(theta),
ylab = expression(f(theta)),
ylim = c(0, max(prior_density) * 1.1))
grid()
qbeta(0.025,alpha=83,beta=27)
qbeta(0.025,83,27)
qbeta(0.5,83,27)
qbeta(0.025,83,27)
qbeta(0.975,83,27)
# Posterior parameters
alpha_post <- 83
beta_post <- 27
# Bayes estimate (posterior mean)
theta_hat <- alpha_post / (alpha_post + beta_post)
# 95% credible interval
ci <- qbeta(c(0.025, 0.975), alpha_post, beta_post)
# Theta values and density
theta_vals <- seq(0, 1, length.out = 1000)
posterior_density <- dbeta(theta_vals, alpha_post, beta_post)
# Plot the posterior density
plot(theta_vals, posterior_density, type = "l", lwd = 2, col = "blue",
main = "Posterior Density: Beta(83, 27)",
xlab = expression(theta), ylab = expression(f(theta)))
# Shade the 95% credible interval
polygon(c(ci[1], theta_vals[theta_vals >= ci[1] & theta_vals <= ci[2]], ci[2]),
c(0, posterior_density[theta_vals >= ci[1] & theta_vals <= ci[2]], 0),
col = rgb(0.2, 0.6, 1, 0.3), border = NA)
# Add vertical line for posterior mean (Bayes estimate)
abline(v = theta_hat, col = "red", lwd = 2, lty = 2)
# Add vertical lines for credible interval bounds
abline(v = ci, col = "darkgreen", lty = 3, lwd = 2)
# Add a legend
legend("topright", legend = c("Posterior Mean", "95% Credible Interval"),
col = c("red", "darkgreen"), lty = c(2, 3), lwd = 2, bty = "n")
# Prior parameters
mu_0 <- 15
sigma_0 <- 2.5
# Range of mu values
mu_vals <- seq(10, 20, length.out = 500)
# Compute the prior density
prior_density <- dnorm(mu_vals, mean = mu_0, sd = sigma_0)
# Plot
plot(mu_vals, prior_density, type = "l", lwd = 2, col = "blue",
main = expression(paste("Prior Density of ", mu, " ~ N(15, 2.5"^2, ")")),
xlab = expression(mu), ylab = expression(f(mu)))
grid()
qnorm(0.99)
qnorm(0.975)
qt(0.05,39)
qnorm(0.05)
library(titymodels)
library(tidymodels)
install.packages("tidymodels")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(tidymodels)
library(tidyverse)
library(tidymodels)
library(tidymodels)
train <- vroom("train.csv")
library(vroom)
train <- vroom("train.csv")
test <- vroom("test.csv")
library(tidymodels)
# Prep and bake the recipe
prepped_recipe <- prep(bike_recipe)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
train <- vroom("train.csv")
#1. Remove casual, registered variables, change count to log(count)
train <- train |>
select(-c(registered, casual)) |>
mutate(count = log1p(count))
setwd("~/Documents/BYU Fall Semester 2025/STAT 348/Competitions/BikeShare")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
train <- vroom("train.csv")
test <- vroom("test.csv")
#1. Remove casual, registered variables, change count to log(count)
train <- train |>
select(-c(registered, casual)) |>
mutate(count = log1p(count))
#2. Feature engineering (define recipe)
bike_recipe <- recipe(count~.,data=train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_time(datetime,features = "hour") %>%
step_mutate(season = factor(season)) %>%
step_date(datetime, features="dow")
my_linear_model <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
## Combine into a Workflow and fit
bike_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(my_linear_model) %>%
fit(data=train)
## Run all the steps on test data
lin_preds <- predict(bike_workflow, new_data = test) %>%
mutate(count = pmax(0, expm1(.pred)))
# Prep and bake the recipe
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = train)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(lubridate)
train <- vroom("train.csv") %>%
mutate(datetime = ymd_hms(datetime))  # ensure POSIXct
test <- vroom("test.csv") %>%
mutate(datetime = ymd_hms(datetime))  # same for test
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log1p(count))
# 2. Feature engineering (define recipe)
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_mutate(season = factor(season)) %>%
step_time(datetime, features = "hour") %>%   # hour of day
step_date(datetime, features = "dow")        # day of week
my_linear_model <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Combine into a Workflow and fit
bike_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(my_linear_model) %>%
fit(data = train)
# Run all the steps on test data
lin_preds <- predict(bike_workflow, new_data = test) %>%
mutate(count = pmax(0, expm1(.pred)))  # back-transform
# Prep and bake the recipe (optional: for inspection)
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = train)
# Kaggle submission file
kaggle_submission <- lin_preds %>%
bind_cols(test) %>%
select(datetime, count) %>%
mutate(datetime = as.character(format(datetime)))  # Kaggle needs string
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(lubridate)
train <- vroom("train.csv") %>%
mutate(datetime = ymd_hms(datetime))  # ensure POSIXct
test <- vroom("test.csv") %>%
mutate(datetime = ymd_hms(datetime))  # same for test
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log1p(count))
# 2. Feature engineering (define recipe)
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_mutate(season = factor(season)) %>%
step_time(datetime, features = "hour") %>%   # hour of day
step_date(datetime, features = "dow")        # day of week
my_linear_model <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Combine into a Workflow and fit
bike_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(my_linear_model) %>%
fit(data = train)
# Run all the steps on test data
lin_preds <- predict(bike_workflow, new_data = test) %>%
mutate(count = pmax(0, expm1(.pred)))  # back-transform
# Prep and bake the recipe (optional: for inspection)
prepped_recipe <- prep(bike_recipe)
baked_train <- bake(prepped_recipe, new_data = train)
# Kaggle submission file
kaggle_submission <- lin_preds %>%
bind_cols(test) %>%
select(datetime, count) %>%
mutate(datetime = as.character(format(datetime)))  # Kaggle needs string
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(lubridate)
train <- vroom("train.csv") %>%
mutate(datetime = ymd_hms(datetime))   # ensure POSIXct
test <- vroom("test.csv") %>%
mutate(datetime = ymd_hms(datetime))   # same for test
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log1p(count))
# 2. Feature engineering (define recipe)
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_mutate(weather = factor(weather)) %>%
step_mutate(season = factor(season)) %>%
step_time(datetime, features = "hour") %>%
step_date(datetime, features = "dow")
my_linear_model <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
# Combine into a Workflow and fit
bike_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(my_linear_model) %>%
fit(data = train)
# Predict on test data and back-transform
lin_preds <- predict(bike_workflow, new_data = test) %>%
mutate(count = pmax(0, expm1(.pred)))
# Build Kaggle submission (must be exactly datetime,count)
kaggle_submission <- test %>%
select(datetime) %>%
mutate(datetime = format(datetime, "%Y-%m-%d %H:%M:%S")) %>% # match Kaggle format
bind_cols(lin_preds %>% select(count))
# Write out the file
vroom_write(kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,   # just pick a small penalty
mixture = 0.5     # 0=ridge, 1=lasso, in between=elastic net
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
library(glmnet)
install.packages("glmnet")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,   # just pick a small penalty
mixture = 0.5     # 0=ridge, 1=lasso, in between=elastic net
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,   # just pick a small penalty
mixture = 0.5     # 0=ridge, 1=lasso, in between=elastic net
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
# Load data and convert datetime
train <- vroom("train.csv") %>%
mutate(datetime = ymd_hms(datetime))
test  <- vroom("test.csv") %>%
mutate(datetime = ymd_hms(datetime))
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,   # just pick a small penalty
mixture = 0.5     # 0=ridge, 1=lasso, in between=elastic net
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# Load data and convert datetime
train <- train %>% mutate(datetime = ymd_hms(datetime))
test  <- test %>% mutate(datetime = ymd_hms(datetime))
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# Load data and convert datetime
train <- train %>% mutate(datetime = ymd_hms(datetime))
test  <- test %>% mutate(datetime = ymd_hms(datetime))
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
# Load data and convert datetime
train <- train %>% mutate(datetime = ymd_hms(datetime))
head(train)
rlang::last_trace()
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# Load data and convert datetime
train <- train %>% mutate(datetime = ymd_hms(datetime))
test  <- test %>% mutate(datetime = ymd_hms(datetime))
# 1. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c(registered, casual)) %>%
mutate(count = log(count))
