library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.data)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- write.csv(drug_consumption.data)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read.csv(drug_consumption.csv)
library(tidyverse)
data <- read_csv(drug_consumption.csv)
library(tidyverse)
data <- read_csv(drug_consumption.csv)
qnorm(0.025)
qnorm(0.025,0.5,0.25/50)
qnorm(0.025,0.5,sqrt(0.25/50))
qnorm(0.975,0.5,sqrt(0.25/50))
pnorm(.361,0.1,sqrt(0.09/50))
1-pnorm(0.639,0.1,sqrt(0.09/50))
pnorm(.361,0.3,sqrt(0.21/50))
pnorm(.361,0.3,sqrt(0.21/50)) + (1-pnorm(0.639,0.3,sqrt(0.21/50)))
pnorm(.361,0.2,sqrt(0.16/50)) + (1-pnorm(0.639,0.2,sqrt(0.16/50)))
qnorm(0.025,0,1)
qnorm(0.025,0,sqrt(1/5))
qnorm(0.975,0,sqrt(1/5))
pnorm(7.66836)
pnorm(7.66)
pnorm(5)
(129.4538-128)/(2.1/sqrt(40))
pnorm(4.3783999)
1-pnorm(4.37)
(129.4538-128)/(2.1/sqrt(40))
pnorm(4.378399)
1-0.999994
1-0.999994)*10
(128.6-128)/(2.1/sqrt(40))
pnorm(1.807016)
pnorm(-1.645)
1-pnorm(4.378)
1-pnorm(4.378)*10
(1-pnorm(4.378))*10
((1-pnorm(4.378))*10)*10
pnorm(-0.4424885)
1-0.3290679
pnorm(-0.4424885)
1-pt(4.378,39)
10*(1-pt(4.378,39))
10*(1-pt(4.38,39))
1-pt(4.568,9)
qnorm(0.01)
qnorm(0.005)
qt(0.005,9)
qt(0.01,9)
pbinom(12,20,0.4)
pbinom(12,20,0.5)
pbinom(12,20,0.6)
pbinom(12,20,0.7)
pbinom(12,20,0.9)
pbinom(12,20,1)
pnorm(0.05)
qnorm(0.05)
1.644854*(sqrt(5/20))
pnorm(7.5,7,sqrt(5))
pnorm(7.5,7,sqrt(5/20))
pnorm(7.5,7.82,sqrt(5/20))
pnorm(8,7.82,sqrt(5/20))
pnorm(8.5,7.82,sqrt(5/20))
pnorm(9,7.82,sqrt(5/20))
qnorm(0.05)
qnorm(0.8)
pnorm(8,7.82,sqrt(5/20))
pnorm(8,7.82,sqrt(5/30))
pnorm(8,7.82,sqrt(5/50))
pnorm(8,7.82,sqrt(5/70))
pnorm(8,7.82,sqrt(5/80))
pnorm(8,7.82,sqrt(5/90))
pnorm(8,7.82,sqrt(5/100))
pnorm(8,7.82,sqrt(5/125))
pnorm(8,7.82,sqrt(5/120))
pnorm(8,7.82,sqrt(5/110))
pnorm(8,7.82,sqrt(5/100))
pnorm(8,7,sqrt(5/31))
# Set up a sequence of theta values between 0 and 1
theta <- seq(0, 1, length.out = 500)
# Compute the density of the Beta(6, 4) distribution
prior_density <- dbeta(theta, shape1 = 6, shape2 = 4)
# Plot the prior density
plot(theta, prior_density, type = "l", lwd = 2, col = "blue",
main = "Prior Density: Beta(6, 4)",
xlab = expression(theta),
ylab = expression(f(theta)),
ylim = c(0, max(prior_density) * 1.1))
grid()
qbeta(0.025,alpha=83,beta=27)
qbeta(0.025,83,27)
qbeta(0.5,83,27)
qbeta(0.025,83,27)
qbeta(0.975,83,27)
# Posterior parameters
alpha_post <- 83
beta_post <- 27
# Bayes estimate (posterior mean)
theta_hat <- alpha_post / (alpha_post + beta_post)
# 95% credible interval
ci <- qbeta(c(0.025, 0.975), alpha_post, beta_post)
# Theta values and density
theta_vals <- seq(0, 1, length.out = 1000)
posterior_density <- dbeta(theta_vals, alpha_post, beta_post)
# Plot the posterior density
plot(theta_vals, posterior_density, type = "l", lwd = 2, col = "blue",
main = "Posterior Density: Beta(83, 27)",
xlab = expression(theta), ylab = expression(f(theta)))
# Shade the 95% credible interval
polygon(c(ci[1], theta_vals[theta_vals >= ci[1] & theta_vals <= ci[2]], ci[2]),
c(0, posterior_density[theta_vals >= ci[1] & theta_vals <= ci[2]], 0),
col = rgb(0.2, 0.6, 1, 0.3), border = NA)
# Add vertical line for posterior mean (Bayes estimate)
abline(v = theta_hat, col = "red", lwd = 2, lty = 2)
# Add vertical lines for credible interval bounds
abline(v = ci, col = "darkgreen", lty = 3, lwd = 2)
# Add a legend
legend("topright", legend = c("Posterior Mean", "95% Credible Interval"),
col = c("red", "darkgreen"), lty = c(2, 3), lwd = 2, bty = "n")
# Prior parameters
mu_0 <- 15
sigma_0 <- 2.5
# Range of mu values
mu_vals <- seq(10, 20, length.out = 500)
# Compute the prior density
prior_density <- dnorm(mu_vals, mean = mu_0, sd = sigma_0)
# Plot
plot(mu_vals, prior_density, type = "l", lwd = 2, col = "blue",
main = expression(paste("Prior Density of ", mu, " ~ N(15, 2.5"^2, ")")),
xlab = expression(mu), ylab = expression(f(mu)))
grid()
qnorm(0.99)
qnorm(0.975)
qt(0.05,39)
qnorm(0.05)
library(titymodels)
library(tidymodels)
install.packages("tidymodels")
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(tidymodels)
library(tidyverse)
library(tidymodels)
library(vroom)
library(vroom)
library(vroom)
library(vroom)
vroom(sampleSubmission.csv)
vroom("sampleSubmission.csv")
sample <- vroom("sampleSubmission.csv")
setwd("~/Documents/BYU Fall Semester 2025/STAT 348/Competitions/BikeShare")
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
View(train)
glimpse(train)
library(tidyverse)
library(tidymodels)
library(vroom)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
glimpse(train)
DataExplorer::plot_bar(train)
install.packages("DataExplorer")
DataExplorer::plot_bar(train)
DataExplorer::plot_bar(train$weather)
DataExplorer::plot_histograms(train$weather)
DataExplorer::plot_histograms(train)
library(patchwork)
library(GGally)
install.packages("GGally")
glimpse(train)
DataExplorer::plot_histograms(train)
ggplot(data=train, aes(x=temp, y=count))
ggplot(data=train, aes(x=temp, y=count)) + geom_point
ggplot((data=train, aes(x=temp, y=count)) + geom_point)
ggplot(data=train, aes(x=temp, y=count)) + geom_point()
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
glimpse(train)
head(train,10)
DataExplorer::plot_bar(train)
dplyr::glimpse(train)
train <- vroom("train.csv",col_types = c(weather="f")
train <- vroom("train.csv",col_types = c(weather="f"))
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
glimpse(train)
ggplot(data=train, aes(x=temp, y=count)) + geom_point()
train$weather <- factor(train[weather], levels = 1:4)
glimpse(train)
ggplot(data=train, aes(x=temp, y=count)) + geom_point()
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train[weather], levels = 1:4)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
glimpse(train)
ggplot(data=train, aes(x=temp, y=count)) + geom_point()
DataExplorer::plot_bar(train)
ggplot(data=train, aes(x=temp, y=count)) + geom_point() +geom_smooth()
ggplot(data=train, aes(x=bar,y=count)) + geom_bar()
ggplot(data=train, aes(x=bar,y=count)) + geom_bar()
ggplot(data=train, aes(x=weather,y=count)) + geom_bar()
ggplot(data=train, aes(x=weather,y=count)) + geom_bar()
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
ggplot(data=train, aes(x=weather) + geom_bar()
ggplot(data=train, aes(x=weather) + geom_bar()
ggplot(data=train, aes(x=weather)) + geom_bar()
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
ggplot(data=train, aes(x=weather)) + geom_bar()
head(train, n=10)
ggplot(data=train, aes(x=datetime, y=count)) + geom_line()
ggplot(data=train, aes(x=datetime, y=count)) + geom_point()
ggplot(data=train, aes(x=casual, y=count)) + geom_point()
ggplot(data=train, aes(x=holiday, y=count)) + geom_point()
ggplot(data=train, aes(x=workingday, y=count)) + geom_point()
train$workingday <-factor(train$workingday, levels = 0:1)
ggplot(data=train, aes(x=workingday, y=count)) + geom_point()
ggplot(data=train, aes(x=workingday, y=count)) + geom_col()
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_density()
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_density()
temperature
view(temperature)
ggplot(data=train, aes(x=temp, y=count)) + geom_density()
ggplot(data=train, aes(x=temp, y=count)) + geom_density()
ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth()
ggplot(train, aes(x = temp, y = count)) +
stat_summary(fun = mean, geom = "point") +
geom_smooth()
ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth()
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
avg_train <- train |>
group_by(weather) |>
summarise(avg_count = mean(count), n_days = n())
ggplot(avg_train, aes(x = weather, y = avg_count)) +
geom_col() +
geom_text(aes(label = n_days), vjust = -0.5)  # add number of days on top
train <- vroom("train.csv")
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
avg_train <- train |>
group_by(weather) |>
summarise(avg_count = mean(count), n_periods = n())
ggplot(avg_train, aes(x = weather, y = avg_count)) +
geom_col() +
geom_text(aes(label = n_periods))
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
train$workingday <-factor(train$workingday, levels = 0:1)
avg_count <- train |>
group_by(workingday) |>
summarise(avg_count = mean(count)
ggplot(avg_workday, aes(x = workingday, y = avg_count)) +
avg_workday <- train |>
group_by(workingday) |>
summarise(avg_count = mean(count), .groups = "drop")
ggplot(avg_workday, aes(x = workingday, y = avg_count)) +
geom_col()
train$holiday <-factor(train$holiday)
avg_holiday <- train |>
group_by(holiday) |>
summarize(avg_count = mean(count))
ggplot(avg_holiday, aes(x=holiday, y = avg_count)) + geom_col()
head(train, n=10)
train$season <- factor(train$season, levels = 1:4)
glimpse(train)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
train$season <- factor(train$season, levels = 1:4)
glimpse(train)
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth()
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
head(train, n=10)
workday <- ggplot(data=train, aes(x=workingday, y=count)) + geom_col()
avg_holiday <- train |>
group_by(holiday) |>
summarize(avg_count = mean(count))
ggplot(avg_holiday, aes(x=holiday, y = avg_count)) + geom_col()
ggplot(data)
ggplot(data=train, aes(x=weather,y=count)) + geom_col()
avg_season <- train |>
group_by(season) |>
summarize(avg_count = mean(count))
ggplot(avg_season, aes(x=season, y = avg_count)) + geom_col()
head(train)
View(train)
View(train)
library(dplyr)
library(ggplot2)
library(lubridate)  # makes working with dates easier
avg_hour <- train |>
mutate(hour = hour(datetime)) |>           # extract hour (0–23)
group_by(hour) |>
summarise(avg_count = mean(count), .groups = "drop")
ggplot(avg_hour, aes(x = hour, y = avg_count)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 0:23) +
labs(x = "Hour of Day", y = "Average Rentals")
libary(patchwork)
install.packages("patchwork")
(temperature + weather) / (season + hours)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
train$season <- factor(train$season, levels = 1:4)
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth() + labs(x="Temparature", y = "Count")
weather <- ggplot(data=train, aes(x=weather,y=count)) + geom_col() + labs(x = "Weather Type", y = "Total Rental Count")
avg_season <- train |>
group_by(season) |>
summarize(avg_count = mean(count))
season <- ggplot(avg_season, aes(x=season, y = avg_count)) + geom_col() + labs(x="Season", y = "Average Count")
library(dplyr)
library(ggplot2)
library(lubridate)
avg_hour <- train |>
mutate(hour = hour(datetime)) |>
group_by(hour) |>
summarise(avg_count = mean(count), .groups = "drop")
hours <- ggplot(avg_hour, aes(x = hour, y = avg_count)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 0:23) +
labs(x = "Hour of Day", y = "Average Rentals")
library(patchwork)
(temperature + weather) / (season + hours)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
train$season <- factor(train$season, levels = 1:4)
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth() + labs(x="Temparature (celsius)", y = "Count")
weather <- ggplot(data=train, aes(x=weather,y=count)) + geom_col() + labs(x = "Weather Type", y = "Total Rental Count")
avg_season <- train |>
group_by(season) |>
summarize(avg_count = mean(count))
season <- ggplot(avg_season, aes(x=season, y = avg_count)) + geom_col() + labs(x="Season", y = "Average Count")
library(dplyr)
library(ggplot2)
library(lubridate)
avg_hour <- train |>
mutate(hour = hour(datetime)) |>
group_by(hour) |>
summarise(avg_count = mean(count), .groups = "drop")
hours <- ggplot(avg_hour, aes(x = hour, y = avg_count)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 0:23) +
labs(x = "Hour of Day", y = "Average Rentals")
library(patchwork)
(temperature + weather) / (season + hours)
library(tidyverse)
library(tidymodels)
library(vroom)
library(patchwork)
library(GGally)
sample <- vroom("sampleSubmission.csv")
train <- vroom("train.csv")
test <- vroom("test.csv")
train$weather <- factor(train$weather, levels = 1:4)
train$season <- factor(
train$season,
levels = 1:4,
labels = c("Spring", "Summer", "Fall", "Winter")
)
temperature <- ggplot(data=train, aes(x=temp, y=count)) + geom_point() + geom_smooth() + labs(x="Temparature (celsius)", y = "Count")
weather <- ggplot(data=train, aes(x=weather,y=count)) + geom_col() + labs(x = "Weather Type", y = "Total Rental Count")
avg_season <- train |>
group_by(season) |>
summarize(avg_count = mean(count))
season <- ggplot(avg_season, aes(x=season, y = avg_count)) + geom_col() + labs(x="Season", y = "Average Count")
library(dplyr)
library(ggplot2)
library(lubridate)
avg_hour <- train |>
mutate(hour = hour(datetime)) |>
group_by(hour) |>
summarise(avg_count = mean(count), .groups = "drop")
hours <- ggplot(avg_hour, aes(x = hour, y = avg_count)) +
geom_line() +
geom_point() +
scale_x_continuous(breaks = 0:23) +
labs(x = "Hour of Day", y = "Average Rentals")
library(patchwork)
(temperature + weather) / (season + hours)
library(tidymodels)
train <- vroom("train.csv")
head(train)
train <- train(select(.-casual,registered))
library(tidymodels)
train <- vroom("train.csv")
train <- train |>
select(-c(registered,casual))
head(train)
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") %>% # Regression just means quantitative response
fit(formula=Count~.-datetime, data=train)
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") %>% # Regression just means quantitative response
fit(formula=count~.-datetime, data=train)
## Generate Predictions Using Linear Model
bike_predictions <- predict(my_linear_model, new_data=test) # Use fit to predict11
bike_predictions ## Look at the output
kaggle_submission <- bike_predictions %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
library(tidymodels)
train <- vroom("train.csv")
test <- vroom("test.csv")
train <- train |>
select(-c(registered,casual))
head(train)
## Setup and Fit the Linear Regression Model
my_linear_model <- linear_reg() %>% #Type of model
set_engine("lm") %>% # Engine = What R function to use
set_mode("regression") %>% # Regression just means quantitative response
fit(formula=count~.-datetime, data=train)
## Generate Predictions Using Linear Model
bike_predictions <- predict(my_linear_model, new_data=test) # Use fit to predict11
bike_predictions ## Look at the output
kaggle_submission <- bike_predictions %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
## Write out the file
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
