add_model(preg_model) %>%
fit(data = train)
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 2. Remove casual, registered variables, change count to log(count)
train <- train %>%
select(-c("registered", "casual")) %>%  # safe even if columns missing
mutate(count = log(count))
# 3. Recipe: encode categoricals + normalize, handle any NAs in factors
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_unknown(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)
# 4. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,   # small penalty
mixture = 0.5     # elastic net
) %>%
set_engine("glmnet")
# 5. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 6. Predict on test and back-transform
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = exp(.pred))
# 7. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and prediction variables
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
# 8. Write out the file
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual"))) %>%
mutate(count = log1p(count))   # <-- key change
# 2. Recipe: encode categoricals + normalize, handle any NAs in factors
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_unknown(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,
mixture = 0.5
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, expm1(.pred)))  # <-- key change
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize, handle any NAs in factors
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_unknown(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,
mixture = 0.5
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,
mixture = 0.5
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.01,
mixture = 0.5
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.75,
mixture = 1
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 50,
mixture = 0.5
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.001,
mixture = 0.3
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
library(tidymodels)
library(tidyverse)
library(dplyr)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables, change count to log1p(count)
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = 0.001,
mixture = 1
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model) %>%
fit(data = train)
# 5. Predict on test and back-transform with expm1
lin_preds <- predict(preg_wf, new_data = test) %>%
mutate(count = pmax(0, .pred))
# 6. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 7. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
library(tidymodels)
library(tidyverse)
library(vroom)
library(glmnet)
library(lubridate)
# 0. Load CSV files
train <- vroom("train.csv")
test  <- vroom("test.csv")
# 1. Remove casual, registered variables
train <- train %>%
select(-any_of(c("registered", "casual")))
# 2. Recipe: encode categoricals + normalize
bike_recipe <- recipe(count ~ ., data = train) %>%
step_mutate(weather = ifelse(weather == 4, 3, weather)) %>%
step_time(datetime, features = "hour") %>%
step_mutate(
hour_sin = sin(2 * pi * datetime_hour / 24),
hour_cos = cos(2 * pi * datetime_hour / 24)
) %>%
step_date(datetime, features = "dow") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_rm(datetime)   # remove original datetime column
# 3. Penalized regression model (glmnet)
preg_model <- linear_reg(
penalty = tune(),
mixture = tune()
) %>%
set_engine("glmnet")
# 4. Workflow
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model)
# 5. Grid of values to tune over
grid_of_tuning_params <- grid_regular(
penalty(),
mixture(),
levels = 5
)
folds <- vfold_cv(data = train, v = 10, repeats = 1)
V_results <- preg_wf %>%
tune_grid(
resamples = folds,
grid = grid_of_tuning_params,
metrics = metric_set(rmse, mae)
)
# 6. Plot Results
collect_metrics(V_results) %>%
filter(.metric == "rmse") %>%
ggplot(aes(x = penalty, y = mean, color = factor(mixture))) +
geom_line()
# 7. Find Best Tuning Parameters
bestTune <- V_results %>%
select_best(metric = "rmse")
# 8. Finalize the Workflow & fit it
final_wf <- preg_wf %>%
finalize_workflow(bestTune) %>%
fit(data = train)
# 9. Predict on test set
lin_preds <- final_wf %>%
predict(new_data = test)
# 10. Kaggle submission (exactly as before)
kaggle_submission <- lin_preds %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(count = pmax(0, count)) %>%
mutate(datetime = as.character(format(datetime)))
# 11. Write out the file
vroom_write(x = kaggle_submission, file = "./LinearPreds.csv", delim = ",")
